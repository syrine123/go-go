{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as sc\n",
    "import sklearn\n",
    "pd.set_option('display.max_columns', 400)\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "from encours_modules import config\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'ubuntu'\n",
    "\n",
    "# Cassandra metadata\n",
    "dev_cassandra_host= 'dtl-cassandra01-d01'\n",
    "dev_cassandra_port= 9200\n",
    "dev_username= 'loaddata'\n",
    "dev_password= 'DecujRiQuigByaibdednofVerr6Odij2'\n",
    "parcours_keyspace = 'parcours'\n",
    "prospect_table = 'referentiel_prospects'\n",
    "valo_retro_one_month_table = 'retro_valo_client_un_mois'\n",
    "valo_retro_one_month_table_test = 'retro_valo_client_un_mois_test'\n",
    "top_prospect_one_month_table = 'top_prospect_encours_un_mois'\n",
    "top_prospect_six_months_table = 'top_prospect_encours_six_mois'\n",
    "score_prospect_encours_table = 'score_prospect_encours'\n",
    "\n",
    "# Elasticsearch hostname\n",
    "dev_es_host= 'dtl-esmaster01-d01'\n",
    "\n",
    "# Elasticsearch port\n",
    "dev_es_port= 9200\n",
    "\n",
    "# Path to pickle one month 'encours' and csv matching files\n",
    "#path_to_one_month_pickle = \"./MODEL/ENCOURS_1MOIS/\"\n",
    "path_to_one_month_pickle = \"/home/foueslat/projet-encours/dtl-encours/MODEL/ENCOURS_1MOIS/\"\n",
    "#path_to_six_months_pickle = \"./MODEL/ENCOURS_6MOIS/\"\n",
    "path_to_six_months_pickle = \"/home/foueslat/projet-encours/dtl-encours/MODEL/ENCOURS_6MOIS/\"\n",
    "\n",
    "# Path to module's parent dir in dev env\n",
    "dev_sys_path = '/home/foueslat/dtl-pnb/'\n",
    "\n",
    "# One month model target name\n",
    "one_month_target_name = 'ENCOURS_1MOIS'\n",
    "six_months_target_name = 'ENCOURS_6MOIS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CassandraClient:\n",
    "\n",
    "    def __init__(self, keyspace):\n",
    "        self.auth_provider = None\n",
    "        self.cluster = None\n",
    "        self.session = None\n",
    "        self.keyspace = keyspace\n",
    "        self.create_session()\n",
    "        self.prepare_statements()\n",
    "\n",
    "    def create_session(self):\n",
    "        self.auth_provider = PlainTextAuthProvider(username=config.dev_username, password=config.dev_password)\n",
    "        self.cluster = Cluster(contact_points = [config.dev_cassandra_host], protocol_version=3, auth_provider=self.auth_provider)\n",
    "        self.session = self.cluster.connect(config.parcours_keyspace)\n",
    "\n",
    "    def get_session(self):\n",
    "        return self.session\n",
    "\n",
    "    def prepare_statements(self):\n",
    "        self.statement_get_prospects = self.session.prepare('select * from ' + config.prospect_table + ' WHERE csp>-1 ALLOW FILTERING')\n",
    "        self.statement_get_valo_retro_one_month = self.session.prepare('select * from ' + config.valo_retro_one_month_table)\n",
    "        self.statement_get_valo_retro_one_month_test = self.session.prepare('select * from ' + config.valo_retro_one_month_table_test)\n",
    "        #+ ' WHERE id_dim_personne=\\'6737208161\\' ALLOW FILTERING'\n",
    "        self.statement_truncate_score_prospect = self.session.prepare('TRUNCATE ' + config.score_prospect_encours_table)\n",
    "        self.statement_insert_six_months_score_encours = self.session.prepare(self.get_statement_insert_score_encours('six_months'))\n",
    "        self.statement_insert_one_months_score_encours = self.session.prepare(self.get_statement_insert_score_encours('one_month'))\n",
    "\n",
    "    def get_statement_insert_score_encours(self, target):\n",
    "        return {\n",
    "            'one_month' : 'INSERT INTO ' + config.score_prospect_encours_table + ' (contactid, score_prospect_encours_six_mois, timestamp, id_dim_temps, cible_encours_six_mois) VALUES (?,?,?,?,?)',\n",
    "            'six_months': 'INSERT INTO ' + config.score_prospect_encours_table + ' (contactid, score_prospect_encours_un_mois, timestamp, id_dim_temps, cible_encours_un_mois) VALUES (?,?,?,?,?)',\n",
    "        }.get(target, 'Unknown target specified')\n",
    "\n",
    "    def insert_one_month_score_encours(self, data_frame):\n",
    "        values = data_frame[[\"contactid\",\"ENCOURS_1MOIS_proba\",\"timestamp\",\"id_dim_temps\",\"predicted_target_ENCOURS_1MOIS\"]].values\n",
    "        for i in range(data_frame.iloc[0:data_frame[\"ENCOURS_1MOIS_proba\"].count()].shape[0]):\n",
    "            bound_stmt = self.statement_insert_one_months_score_encours.bind([values[i,0],values[i,1],values[i,2],values[i,3],values[i,4]])\n",
    "            self.session.execute(bound_stmt)\n",
    "\n",
    "    def insert_six_month_score_encours(self, data_frame):\n",
    "        values = data_frame[[\"contactid\",\"ENCOURS_6MOIS_proba\",\"timestamp\",\"id_dim_temps\",\"predicted_target_ENCOURS_6MOIS\"]].values\n",
    "        for i in range(data_frame.iloc[0:data_frame[\"ENCOURS_6MOIS_proba\"].count()].shape[0]):\n",
    "            bound_stmt = self.statement_insert_six_months_score_encours.bind([values[i,0],values[i,1],values[i,2],values[i,3],values[i,4]])\n",
    "            self.session.execute(bound_stmt)\n",
    "\n",
    "    def get_df_from_query(self,prepared_query):\n",
    "        resultset = self.session.execute(prepared_query)\n",
    "        resultset[0]\n",
    "        df = pd.DataFrame()\n",
    "        for num_col in range(len(resultset.column_names)):\n",
    "            result_col = []\n",
    "            for row in resultset:\n",
    "                result_col.append(row[num_col])\n",
    "            df[resultset.column_names[num_col]] = result_col\n",
    "        return df\n",
    "\n",
    "    def get_df_from_query2(self,prepared_query):\n",
    "        resultset = self.session.execute(prepared_query)\n",
    "        resultset[0]\n",
    "        df = pd.DataFrame()\n",
    "        for r in resultset:\n",
    "            df = df.append(r)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc = CassandraClient(parcours_keyspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:cassandra.cluster:Using index operator on paged results causes entire result set to be materialized.\n"
     ]
    }
   ],
   "source": [
    "prospects_ref = cc.get_df_from_query(cc.statement_get_prospects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "retro_encours_one_month = cc.get_df_from_query(cc.statement_get_valo_retro_one_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#retro_encours_one_month['id_dim_personne'].to_csv(\"/home/foueslat/liste_id_clients.csv\", sep=\";\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = pd.merge(prospects_ref,retro_encours_one_month, right_on='id_dim_personne', left_on='contactid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contactid                    4114\n",
       "annee_mois_x                  167\n",
       "campagne                     4113\n",
       "campaign                     4113\n",
       "civilite                     4114\n",
       "code_postal                  4113\n",
       "country_connexion_name       4113\n",
       "csp                          4114\n",
       "date_naissance               4113\n",
       "debit_cb                     3619\n",
       "firstnamesponsor             2288\n",
       "flag_banque_principale       4114\n",
       "flag_epargne                 4114\n",
       "id_dim_temps_x               4114\n",
       "mail                         4113\n",
       "mailing_accord               4113\n",
       "namesponsor                  2288\n",
       "nature_cb                    3619\n",
       "patrimoine                   4114\n",
       "pays                         4113\n",
       "regime_matrimonial           4114\n",
       "revenus_annuels              4114\n",
       "service                      4114\n",
       "timestamp                    4114\n",
       "id_dim_personne              4114\n",
       "annee_mois_y                 4114\n",
       "annee_mois_m_x               4114\n",
       "cible_encours_predit         4114\n",
       "cible_reelle                 4114\n",
       "date_entree_relation         4114\n",
       "encours                      3296\n",
       "id_dim_temps_y               4114\n",
       "identifiant_web              4114\n",
       "score_prospect_encours       4114\n",
       "x_mois_anterieurs_cumules    4114\n",
       "dtype: int64"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.head()\n",
    "merged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### USELESS COLUMNS\n",
    "del merged['annee_mois_m_x']\n",
    "del merged['annee_mois_x']\n",
    "del merged['annee_mois_y']\n",
    "del merged['firstnamesponsor']\n",
    "del merged['id_dim_temps_x']\n",
    "del merged['id_dim_temps_y']\n",
    "del merged['id_dim_personne']\n",
    "del merged['cible_encours_predit']\n",
    "del merged['date_entree_relation']\n",
    "del merged['encours']\n",
    "del merged['score_prospect_encours']\n",
    "del merged['x_mois_anterieurs_cumules']\n",
    "del merged['timestamp']\n",
    "del merged['service']\n",
    "del merged['identifiant_web']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEL Columns with potential\n",
    "del merged['mail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged['cible_un_mois'] = merged['cible_reelle']\n",
    "del merged['cible_reelle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contactid                 4114\n",
       "campagne                  4113\n",
       "campaign                  4113\n",
       "civilite                  4114\n",
       "code_postal               4113\n",
       "country_connexion_name    4113\n",
       "csp                       4114\n",
       "date_naissance            4113\n",
       "debit_cb                  3619\n",
       "flag_banque_principale    4114\n",
       "flag_epargne              4114\n",
       "mailing_accord            4113\n",
       "namesponsor               2288\n",
       "nature_cb                 3619\n",
       "patrimoine                4114\n",
       "pays                      4113\n",
       "regime_matrimonial        4114\n",
       "revenus_annuels           4114\n",
       "cible_un_mois             4114\n",
       "dtype: int64"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "infos_naissance = pd.read_csv('/home/foueslat/projet-encours/oo', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged.contactid = pd.to_numeric(merged.contactid, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set = pd.merge(merged,infos_naissance, right_on='CONTACT_ID', left_on='contactid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contactid                         4114\n",
       "campagne                          4113\n",
       "campaign                          4113\n",
       "civilite                          4114\n",
       "code_postal                       4113\n",
       "country_connexion_name            4113\n",
       "csp                               4114\n",
       "date_naissance                    4113\n",
       "debit_cb                          3619\n",
       "flag_banque_principale            4114\n",
       "flag_epargne                      4114\n",
       "mailing_accord                    4113\n",
       "namesponsor                       2288\n",
       "nature_cb                         3619\n",
       "patrimoine                        4114\n",
       "pays                              4113\n",
       "regime_matrimonial                4114\n",
       "revenus_annuels                   4114\n",
       "cible_un_mois                     4114\n",
       "CONTACT_ID                        4114\n",
       "Pays De Naissance                 4114\n",
       "Code Postal Ville de Naissance    4114\n",
       "Lieu Naissance                    4114\n",
       "dtype: int64"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_set.to_csv('/home/foueslat/data_set_with_birth.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEL sur data_set (résultat de retro_un_mois innerjoin ref_prospects innerjoin infos_naissance)\n",
    "del data_set['Lieu Naissance']\n",
    "data_set['code_postal_naissance'] = data_set['Code Postal Ville de Naissance'] \n",
    "data_set['pays_naissance_iso'] = data_set['Pays De Naissance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contactid                         4114\n",
       "campagne                          4113\n",
       "campaign                          4113\n",
       "civilite                          4114\n",
       "code_postal                       4113\n",
       "country_connexion_name            4113\n",
       "csp                               4114\n",
       "date_naissance                    4113\n",
       "debit_cb                          3619\n",
       "flag_banque_principale            4114\n",
       "flag_epargne                      4114\n",
       "mailing_accord                    4113\n",
       "namesponsor                       2288\n",
       "nature_cb                         3619\n",
       "patrimoine                        4114\n",
       "pays                              4113\n",
       "regime_matrimonial                4114\n",
       "revenus_annuels                   4114\n",
       "cible_un_mois                     4114\n",
       "CONTACT_ID                        4114\n",
       "Pays De Naissance                 4114\n",
       "Code Postal Ville de Naissance    4114\n",
       "code_postal_naissance             4114\n",
       "pays_naissance_iso                4114\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def datamanagement(X,cible,chemin):\n",
    "    \n",
    "    #*************************** I- creation d un lieu de sauvgarde du datamanagement ******************\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    plus0=\"/\"\n",
    "    plus1=cible\n",
    "    plus2=\"MOD\"\n",
    "    chemin1=chemin+plus0+plus2+plus0+plus1+plus0\n",
    "    \n",
    "    if os.path.isdir(chemin1)==True:\n",
    "        os.chdir(chemin1)\n",
    "    else:\n",
    "        os.makedirs(chemin1) \n",
    "        os.chdir(chemin1)\n",
    "    \n",
    " \n",
    "    C = pd.DataFrame(X.columns, columns=['level_1'])\n",
    "    C.to_csv(\"VARIABLE.csv\",header=True, index=False,sep=';')\n",
    "    \n",
    "    #*************************** II- retaitement manuel de variables ******************\n",
    "    \n",
    "    from datetime import datetime\n",
    "    X['civilite']=X['civilite'].replace(np.nan,'NR')\n",
    "    X['code_postal']=X['code_postal'].replace('Vide','-2')\n",
    "    X['code_postal']=X['code_postal'].replace(np.nan,'-2')\n",
    "    #X['code_postal_naissance']=X['code_postal_naissance'].replace(np.nan,'-2')\n",
    "\n",
    "\n",
    "    now=datetime.now()\n",
    "    X['AGE']=X['date_naissance'].replace(np.nan,'01/01/1800')\n",
    "    X['AGE']= pd.to_datetime(X['AGE'],errors='coerce')\n",
    "    X['AGE']=X['AGE'].map(lambda x : now.year-x.year  if now.month-x.month>0 else now.year - x.year -1 )\n",
    "    del X['date_naissance']\n",
    "    \n",
    "    X['debit_cb']=X['debit_cb'].replace(np.nan,'SANS_CB')\n",
    "    X['nature_cb']=X['nature_cb'].replace(np.nan,'SANS_CB')\n",
    "    \n",
    "    X['Parrain']=X['namesponsor'].map(lambda x : 0 if x == np.nan else 1 )\n",
    "    del X['namesponsor']\n",
    "    \n",
    "    X['pays']=X['pays'].replace(np.nan,'NR')\n",
    "    #X['pays_naissance_iso']=X['pays_naissance_iso'].replace(np.nan,'NR')\n",
    "\n",
    "    nb=-2\n",
    "    X['regime_matrimonial']=X['regime_matrimonial'].fillna(nb)\n",
    "    X['patrimoine']=X['patrimoine'].fillna(nb)\n",
    "    X['revenus_annuels']=X['revenus_annuels'].fillna(nb)\n",
    "    \n",
    "    #X[cible]=X['LABEL'].map(lambda x :1 if x==4 else 0)\n",
    "  \n",
    "    #del X['LABEL']\n",
    "    \n",
    "    #*************************** III- transformation de donnees ******************\n",
    "        \n",
    "    #3.1. pour donnees quanti ou int on fait un cox box pour quali on regroup \n",
    "    columns1 = set(X.columns)\n",
    "    columns1.remove(cible)\n",
    "  \n",
    "    import scipy.stats as sc\n",
    "    for col in columns1:\n",
    "        if X[col].dtype == np.dtype('float64') and len(np.unique(X[col].values))>=50:\n",
    "            param=X[col].min()\n",
    "            param2=pd.DataFrame({'valeur':[param]})\n",
    "            param2.to_csv(col+\"_param.csv\",sep=';',encoding='utf-8')\n",
    "            if param>=0:\n",
    "                transformed_y, lambda_ = sc.boxcox(X[col]+1)\n",
    "                export=pd.DataFrame({'valeur':[lambda_]})\n",
    "                export.to_csv(col+\"_coxbox.csv\",sep=';',encoding='utf-8')\n",
    "                if lambda_>= 0.0 and lambda_<= 0.09:\n",
    "                    X[col]=X[col].map(lambda x : log(x+1))\n",
    "                else:\n",
    "                    X[col]=X[col].map(lambda x : ((x+1)**lambda_ -1)/lambda_)\n",
    "            else:\n",
    "                transformed_y, lambda_ = sc.boxcox(X[col]-param+1)\n",
    "                export=pd.DataFrame({'valeur':[lambda_]})\n",
    "                export.to_csv(col+\"_coxbox.csv\",sep=';',encoding='utf-8')\n",
    "                if lambda_>= 0.0 and lambda_<= 0.09:\n",
    "                    X[col]=X[col].map(lambda x : log(x-param+1))\n",
    "                else:\n",
    "                    X[col]=X[col].map(lambda x : ((x-param+1)**lambda_ -1)/lambda_)\n",
    "                     \n",
    "                \n",
    "        if X[col].dtype == np.dtype('int64') and len(np.unique(X[col].values))>=50:\n",
    "            param=X[col].min()\n",
    "            param2=pd.DataFrame({'valeur':[param]})\n",
    "            param2.to_csv(col+\"_param.csv\",sep=';',encoding='utf-8')\n",
    "            if param>=0:\n",
    "                transformed_y, lambda_ = sc.boxcox(X[col]+1)\n",
    "                export=pd.DataFrame({'valeur':[lambda_]})\n",
    "                export.to_csv(col+\"_coxbox.csv\",sep=';',encoding='utf-8')\n",
    "                if lambda_>= 0.0 and lambda_<= 0.09:\n",
    "                    X[col]=X[col].map(lambda x : log(x+1))\n",
    "                else:\n",
    "                    X[col]=X[col].map(lambda x : ((x+1)**lambda_ -1)/lambda_)\n",
    "            else:\n",
    "                transformed_y, lambda_ = sc.boxcox(X[col]-param+1)\n",
    "                export=pd.DataFrame({'valeur':[lambda_]})\n",
    "                export.to_csv(col+\"_coxbox.csv\",sep=';',encoding='utf-8')\n",
    "                if lambda_>= 0.0 and lambda_<= 0.09:\n",
    "                    X[col]=X[col].map(lambda x : log(x-param+1))\n",
    "                else:\n",
    "                    X[col]=X[col].map(lambda x : ((x-param+1)**lambda_ -1)/lambda_)\n",
    "        \n",
    "        if X[col].dtype == np.dtype('object') :\n",
    "            if len(np.unique(X[col].values))<=20:\n",
    "                regroup=pd.DataFrame(X[col].value_counts(normalize = True))\n",
    "                regroup2=regroup[regroup[col]>=0.05].index\n",
    "                X[col]=X[col].map(lambda x : x if x in regroup2 else 'AUTRE')\n",
    "                le = preprocessing.LabelEncoder()\n",
    "                G=pd.DataFrame(X[col])\n",
    "                G['valeur']=le.fit_transform(G[col])\n",
    "                L3=G.groupby(['valeur',col])[col].count()\n",
    "                L3.to_csv(col+\".csv\",sep=';',encoding='utf-8')\n",
    "                X[col]=le.fit_transform(X[col])\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                regroup=pd.DataFrame(X[col].value_counts(normalize = True))\n",
    "                if regroup[col].max()<=0.05:\n",
    "                    le = preprocessing.LabelEncoder()\n",
    "                    G=pd.DataFrame(X[col])\n",
    "                    G['valeur']=le.fit_transform(G[col])\n",
    "                    L3=G.groupby(['valeur',col])[col].count()\n",
    "                    L3.to_csv(col+\".csv\",sep=';',encoding='utf-8')\n",
    "                    X[col]=le.fit_transform(X[col])\n",
    "                else:\n",
    "                    regroup2=regroup[regroup[col]>=0.03].index\n",
    "                    X[col]=X[col].map(lambda x : x if x in regroup2 else 'AUTRE')\n",
    "                    le = preprocessing.LabelEncoder()\n",
    "                    G=pd.DataFrame(X[col])\n",
    "                    G['valeur']=le.fit_transform(G[col])\n",
    "                    L3=G.groupby(['valeur',col])[col].count()\n",
    "                    L3.to_csv(col+\".csv\",sep=';',encoding='utf-8')\n",
    "                    X[col]=le.fit_transform(X[col])\n",
    "                \n",
    "    \n",
    "    #3.2.Traitement des valeurs null     \n",
    "    columns2 = set(X.columns)\n",
    "    columns2.remove(cible)\n",
    "    for col in columns2:\n",
    "        if X[col].dtype == np.dtype('int64')and len(np.unique(X[col].values))>=50:\n",
    "            median=X[col].median()\n",
    "            export=pd.DataFrame({'valeur':[median]})\n",
    "            export.to_csv(col+\"_gestiondesvides.csv\",sep=';',encoding='utf-8')\n",
    "            X[col]=X[col].fillna(median)\n",
    "        if X[col].dtype == np.dtype('int64')and len(np.unique(X[col].values))<50:\n",
    "            mostfreq=pd.DataFrame(X[col].value_counts(normalize = True))\n",
    "            mfreq=float(mostfreq.max())\n",
    "            export=pd.DataFrame({'valeur':[mfreq]})\n",
    "            export.to_csv(col+\"_gestiondesvides.csv\",sep=';',encoding='utf-8')\n",
    "            X[col]=X[col].fillna(mfreq)\n",
    "        if X[col].dtype == np.dtype('float64')and len(np.unique(X[col].values))>=50:\n",
    "            median=X[col].median()\n",
    "            export=pd.DataFrame({'valeur':[median]})\n",
    "            export.to_csv(col+\"_gestiondesvides.csv\",sep=';',encoding='utf-8')\n",
    "            X[col]=X[col].fillna(median)\n",
    "        if X[col].dtype == np.dtype('float64')and len(np.unique(X[col].values))<50:\n",
    "            mostfreq=pd.DataFrame(X[col].value_counts(normalize = True))\n",
    "            mfreq=float(mostfreq.max())\n",
    "            export=pd.DataFrame({'valeur':[mfreq]})\n",
    "            export.to_csv(col+\"_gestiondesvides.csv\",sep=';',encoding='utf-8')\n",
    "            X[col]=X[col].fillna(mfreq)\n",
    "    \n",
    "    par=list(X.columns)\n",
    "    par.sort()\n",
    "    X=pd.DataFrame(X,columns=par)\n",
    "    to_drop=[cible]\n",
    "    return X.drop(to_drop,axis=1),X[cible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged.to_csv(\"/home/foueslat/data_set.csv\", sep=\";\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contactid                   int64\n",
       "campagne                  float64\n",
       "campaign                   object\n",
       "civilite                   object\n",
       "code_postal                object\n",
       "country_connexion_name     object\n",
       "csp                       float64\n",
       "date_naissance             object\n",
       "debit_cb                   object\n",
       "flag_banque_principale    float64\n",
       "flag_epargne               object\n",
       "mailing_accord             object\n",
       "namesponsor                object\n",
       "nature_cb                  object\n",
       "patrimoine                float64\n",
       "pays                       object\n",
       "regime_matrimonial        float64\n",
       "revenus_annuels           float64\n",
       "cible_un_mois               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([             u'contactid',               u'campagne',\n",
       "                     u'campaign',               u'civilite',\n",
       "                  u'code_postal', u'country_connexion_name',\n",
       "                          u'csp',         u'date_naissance',\n",
       "                     u'debit_cb', u'flag_banque_principale',\n",
       "                 u'flag_epargne',         u'mailing_accord',\n",
       "                  u'namesponsor',              u'nature_cb',\n",
       "                   u'patrimoine',                   u'pays',\n",
       "           u'regime_matrimonial',        u'revenus_annuels',\n",
       "                u'cible_un_mois'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.set_index(['contactid'])\n",
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del data_set['contactid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,y=datamanagement(merged.copy(),'cible_un_mois','/home/foueslat/projet-encours/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################# II- modelisation: construction du modele  ######################################\n",
    "def modelisation (X,y,cible,chemin):  \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    u = cible\n",
    "    v =\"proba\"\n",
    "    probabilite =\"%s_%s\" %(u,v)\n",
    "    \n",
    "    #***********************************I-indicateurs de mesure de performence***********************\n",
    "    \n",
    "    def mesure(table,cible):\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import os\n",
    "        \n",
    "        u= cible\n",
    "        v=\"proba\"\n",
    "        probabilite=\"%s_%s\" %(u,v)\n",
    "        table.sort_values(by=probabilite,ascending=True)\n",
    "        decoup=pd.qcut(table[probabilite],10,labels=[9,8,7,6,5,4,3,2,1,0])\n",
    "        U=pd.DataFrame(decoup)\n",
    "        U.rename(columns={probabilite:\"decile\"},inplace=True)\n",
    "        decile=pd.concat([table,U],axis=1)\n",
    "        T1=decile[cible].groupby(decile['decile'])\n",
    "        T2=pd.DataFrame(T1.mean())\n",
    "        T2.rename(columns={probabilite:\"taux_cible\"},inplace=True)\n",
    "        T2=T2.reset_index()\n",
    "        result=(T2[T2['decile']==0][cible]/table[cible].mean())\n",
    "        return result\n",
    "\n",
    "    def proba_reco(table,cible):\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import os\n",
    "\n",
    "        u= cible\n",
    "        v=\"proba\"\n",
    "        probabilite=\"%s_%s\" %(u,v)\n",
    "        table.sort_values(by=probabilite,ascending=True)\n",
    "        decoup=pd.qcut(table[probabilite],10,labels=[9,8,7,6,5,4,3,2,1,0])\n",
    "        U=pd.DataFrame(decoup)\n",
    "        U.rename(columns={probabilite:\"decile\"},inplace=True)\n",
    "        decile=pd.concat([table,U],axis=1)\n",
    "        T1=decile[probabilite].groupby(decile['decile'])\n",
    "        T2=pd.DataFrame(T1.min())\n",
    "        T2.rename(columns={probabilite:\"proba_min_decile\"},inplace=True)\n",
    "        T2=T2.reset_index()\n",
    "        proba=(T2[T2['decile']==0][\"proba_min_decile\"])\n",
    "        return proba\n",
    "    \n",
    "    #***********************************II-modelisation ***************************************** \n",
    "    \n",
    "    #1.0.echantillionnage\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    X0_train,X0_test,y0_train,y0_test=train_test_split(X,y,random_state=42)\n",
    "    \n",
    "    #1.1. modules necessaire\n",
    "    \n",
    "    from sklearn import linear_model,decomposition\n",
    "    from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, jaccard_similarity_score, precision_score, f1_score, recall_score\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.feature_selection import RFE,SelectFromModel\n",
    "    from sklearn import metrics\n",
    "    from sklearn import  grid_search\n",
    "    from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier,BaggingClassifier\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    from sklearn import svm\n",
    "    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    #1.2. modele 1\n",
    "    try:\n",
    "        parameters = {'n_estimators':[150,300,350], 'n_jobs':[-1]}\n",
    "        rfc= RandomForestClassifier()\n",
    "        m0=grid_search.GridSearchCV(rfc,parameters,cv=6,n_jobs=-1)\n",
    "        m0 = m0.fit(X0_train,y0_train)\n",
    "        proba0=pd.DataFrame(y0_test)\n",
    "        P = m0.predict_proba(X0_test)[:, 1]\n",
    "        y_pred = m0.predict(X0_test)\n",
    "        proba0[probabilite]= P\n",
    "        R0=mesure(proba0,cible)\n",
    "        R0=pd.DataFrame(R0)\n",
    "        R0.rename(columns={cible:\"R0\"},inplace=True)\n",
    "        print(R0)\n",
    "        f = f1_score(y0_test, y_pred)\n",
    "        print(f)\n",
    "        print(\"OTOTOFOFOFO\")\n",
    "        print(accuracy_score(y0_test, y_pred))\n",
    "        print(precision_score(y0_test, y_pred))\n",
    "        \n",
    "        \n",
    "    except ValueError:\n",
    "        R0=pd.DataFrame([0],index=[9],columns=[\"R0\"])\n",
    "        print(R0)\n",
    "\n",
    "    #1.3. modele 2\n",
    "    try:\n",
    "        parameters = {'n_estimators':[300,400], 'n_jobs':[-1]}\n",
    "        rfc= ExtraTreesClassifier()\n",
    "        m1=grid_search.GridSearchCV(rfc,parameters,cv=6,n_jobs=-1)\n",
    "        m1 = m1.fit(X0_train,y0_train)\n",
    "        proba1=pd.DataFrame(y0_test)\n",
    "        P = m1.predict_proba(X0_test)[:, 1]\n",
    "        proba1[probabilite]= P\n",
    "        R1=mesure(proba1,cible)\n",
    "        R1=pd.DataFrame(R1)\n",
    "        R1.rename(columns={cible:\"R1\"},inplace=True)\n",
    "        print(R1)\n",
    "        \n",
    "    except ValueError:\n",
    "        R1=pd.DataFrame([0],index=[9],columns=[\"R1\"])\n",
    "        print(R1)\n",
    "    \n",
    "    #1.4. modele 3\n",
    "    try:\n",
    "        parameters = {'n_estimators':[150,300,350]}\n",
    "        rfc= GradientBoostingClassifier(max_depth=7)\n",
    "        m2=grid_search.GridSearchCV(rfc,parameters,cv=6)\n",
    "        m2 = m2.fit(X0_train,y0_train)\n",
    "        P=m2.predict(X0_test)\n",
    "        proba2=pd.DataFrame(y0_test)\n",
    "        P = m2.predict_proba(X0_test)[:, 1]\n",
    "        proba2[probabilite]= P\n",
    "        R2=mesure(proba2,cible)\n",
    "        R2=pd.DataFrame(R2)\n",
    "        R2.rename(columns={cible:\"R2\"},inplace=True)\n",
    "        print(R2)\n",
    "        f = f1_score(y0_test, y_pred)\n",
    "        print(f)\n",
    "        print(\"OTOTOFOFOFO\")\n",
    "        print(accuracy_score(y0_test, y_pred))\n",
    "        print(precision_score(y0_test, y_pred))\n",
    "    \n",
    "        \n",
    "    except ValueError:\n",
    "        R2=pd.DataFrame([0],index=[9],columns=[\"R2\"])\n",
    "        print(R2)\n",
    "    \n",
    "    #1.5.modele 4\n",
    "    try:\n",
    "        rfc= RandomForestClassifier(n_estimators=200, n_jobs=-1)\n",
    "        logistic = linear_model.LogisticRegression(solver='liblinear',C=100,n_jobs=-1)\n",
    "        select=SelectFromModel(logistic,threshold=\"median\")\n",
    "        a0 = CalibratedClassifierCV(rfc, cv=6, method='isotonic')\n",
    "        m3 = Pipeline(steps=[('feature_selection', select),('regression', a0)])\n",
    "        m3 = m3.fit(X0_train,y0_train)\n",
    "        P = m3.predict_proba(X0_test)[:, 1]\n",
    "        proba3=pd.DataFrame(y0_test)\n",
    "        proba3[probabilite]= P\n",
    "        R3=mesure(proba3,cible)\n",
    "        R3=pd.DataFrame(R3)\n",
    "        R3.rename(columns={cible:\"R3\"},inplace=True)\n",
    "        print(R3)\n",
    "    \n",
    "    except ValueError:\n",
    "        R3=pd.DataFrame([0],index=[9],columns=[\"R3\"])\n",
    "        print(R3)\n",
    "        \n",
    "    #1.6.modele 5\n",
    "    try:\n",
    "        rfc= GradientBoostingClassifier(n_estimators=200,max_depth=8)\n",
    "        logistic = linear_model.LogisticRegression(solver='liblinear',C=100,n_jobs=-1)\n",
    "        select=SelectFromModel(logistic,threshold=\"median\")\n",
    "        a0 = CalibratedClassifierCV(rfc, cv=6, method='sigmoid')\n",
    "        m4 = Pipeline(steps=[('feature_selection', select),('regression', a0)])\n",
    "        m4 = m4.fit(X0_train,y0_train)\n",
    "        P = m4.predict_proba(X0_test)[:, 1]\n",
    "        proba4=pd.DataFrame(y0_test)\n",
    "        proba4[probabilite]= P\n",
    "        R4=mesure(proba4,cible)\n",
    "        R4=pd.DataFrame(R4)\n",
    "        R4.rename(columns={cible:\"R4\"},inplace=True)\n",
    "        print(R4)\n",
    "    \n",
    "    except ValueError:\n",
    "        R4=pd.DataFrame([0],index=[9],columns=[\"R4\"])\n",
    "        print(R4)\n",
    "    \n",
    "    #1.7.modele 6\n",
    "    try:\n",
    "        etc= ExtraTreesClassifier(n_estimators=400,max_depth=10,n_jobs=-1)\n",
    "        m5=etc.fit(X0_train,y0_train)\n",
    "        P = m5.predict_proba(X0_test)[:, 1]\n",
    "        proba5=pd.DataFrame(y0_test)\n",
    "        proba5[probabilite]= P\n",
    "        R5=mesure(proba5,cible)\n",
    "        R5=pd.DataFrame(R5)\n",
    "        R5.rename(columns={cible:\"R5\"},inplace=True)\n",
    "        print(R5)\n",
    "    \n",
    "    except ValueError:\n",
    "        R5=pd.DataFrame([0],index=[9],columns=[\"R5\"])\n",
    "        print(R5)\n",
    "    \n",
    "    #1.8.modele 7\n",
    "    try:\n",
    "        parameters = {'n_estimators':[200,300], 'max_depth':[10,18],'n_jobs':[-1]}\n",
    "        rfc= RandomForestClassifier()\n",
    "        m6=grid_search.GridSearchCV(rfc,parameters,cv=6,n_jobs=-1).fit(X0_train,y0_train)\n",
    "        proba6=pd.DataFrame(y0_test)\n",
    "        P = m6.predict_proba(X0_test)[:, 1]\n",
    "        proba6[probabilite]= P\n",
    "        R6=mesure(proba6,cible)\n",
    "        R6=pd.DataFrame(R6)\n",
    "        R6.rename(columns={cible:\"R6\"},inplace=True)\n",
    "        print(R6)\n",
    "        \n",
    "    except ValueError:\n",
    "        R6=pd.DataFrame([0],index=[9],columns=[\"R6\"])\n",
    "        print(R6)\n",
    "       \n",
    "    try:\n",
    "        coef=[0.14,0.14,0.14,0.14,0.14,0.14,0.14]\n",
    "        proba7=coef[0]*proba0+coef[1]*proba1+coef[2]*proba2+coef[3]*proba3+coef[4]*proba4+coef[5]*proba5+coef[6]*proba6\n",
    "        R7=mesure(proba7,cible)\n",
    "        R7=pd.DataFrame(R7)\n",
    "        R7.rename(columns={cible:\"R7\"},inplace=True)\n",
    "        coef2=coef\n",
    "        coef=pd.DataFrame(coef2,columns=['valeur'])\n",
    "        coef.to_csv(\"coefmodel.csv\",sep=';',encoding='utf-8')\n",
    "        print(R7)\n",
    "    except ValueError:\n",
    "        R7=pd.DataFrame([0],index=[9],columns=[\"R7\"])\n",
    "        print(R7)\n",
    "    \n",
    "    try:\n",
    "        R9=R7\n",
    "        R9.rename(columns={\"R7\":\"R8\"},inplace=True)\n",
    "        \n",
    "        for i in xrange(1,100000):\n",
    "            li=list(np.random.random(7))\n",
    "            li=li/sum(li)\n",
    "            coef=list(li)\n",
    "            proba8=coef[0]*proba0+coef[1]*proba1+coef[2]*proba2+coef[3]*proba3+coef[4]*proba4+coef[5]*proba5+coef[6]*proba6\n",
    "            R8=mesure(proba8,cible)\n",
    "            R8=pd.DataFrame(R8)\n",
    "            R8.rename(columns={cible:\"R8\"},inplace=True)\n",
    "            if float(R8[\"R8\"])>float(R9[\"R8\"]):\n",
    "                R9=R8\n",
    "                coef2=coef\n",
    "        coef=pd.DataFrame(coef2,columns=['valeur'])\n",
    "        coef.to_csv(\"coefmodel.csv\",sep=';',encoding='utf-8')\n",
    "        print(R9)\n",
    "    except ValueError:\n",
    "        R8=pd.DataFrame([0],index=[9],columns=[\"R8\"])\n",
    "        print(R8)\n",
    "        \n",
    "     \n",
    "    #***********************************III-choix du modele *****************************************\n",
    "    import cPickle\n",
    "    plus0=\"/\"\n",
    "    plus1=cible\n",
    "    plus2=\"MOD\"\n",
    "    chemin1=chemin+plus2+plus0+plus1+plus0\n",
    "    \n",
    "    if os.path.isdir(chemin1)==True:\n",
    "        os.chdir(chemin1)\n",
    "    else:\n",
    "        os.makedirs(chemin1) \n",
    "        os.chdir(chemin1)\n",
    "\n",
    "        \n",
    "    R=pd.concat([R0,R1,R2,R3,R4,R5,R6,R7],axis=1)\n",
    "    RR=R.T\n",
    "    RR=pd.DataFrame(RR)\n",
    "    RR.rename(columns=lambda x: 'valeur',inplace=True)\n",
    "    RR.sort_values(by='valeur',ascending=False,inplace=True)\n",
    "    RRR=pd.DataFrame(RR.head(1))\n",
    "    param=RRR.index\n",
    "\n",
    "     \n",
    "    if param==\"R0\":\n",
    "        with open(chemin1+\"MODELE.pickle\",'wb') as f:\n",
    "            cPickle.dump(m0,f)\n",
    "        export=proba_reco(proba0,cible)\n",
    "        print(export)\n",
    "        export.to_csv(chemin1+cible+\"_reco_proba.csv\",sep=';',encoding='utf-8')   \n",
    "    if param==\"R1\":\n",
    "        with open(chemin1+\"MODELE.pickle\",'wb') as f:\n",
    "            cPickle.dump(m1,f)\n",
    "        export=proba_reco(proba1,cible)\n",
    "        print(export)\n",
    "        export.to_csv(chemin1+cible+\"_reco_proba.csv\",sep=';',encoding='utf-8')\n",
    "    if param==\"R2\":\n",
    "        with open(chemin1+\"MODELE.pickle\",'wb') as f:\n",
    "            cPickle.dump(m2,f)\n",
    "        export=proba_reco(proba2,cible)\n",
    "        print(export)\n",
    "        export.to_csv(chemin1+cible+\"_reco_proba.csv\",sep=';',encoding='utf-8')\n",
    "        \n",
    "    if param==\"R3\":\n",
    "        with open(chemin1+\"MODELE.pickle\",'wb') as f:\n",
    "            cPickle.dump(m3,f)\n",
    "        export=proba_reco(proba3,cible)\n",
    "        print(export)\n",
    "        export.to_csv(chemin1+cible+\"_reco_proba.csv\",sep=';',encoding='utf-8')\n",
    "        \n",
    "    if param==\"R4\":\n",
    "        with open(chemin1+\"MODELE.pickle\",'wb') as f:\n",
    "            cPickle.dump(m4,f)\n",
    "        export=proba_reco(proba4,cible)\n",
    "        print(export)\n",
    "        export.to_csv(chemin1+cible+\"_reco_proba.csv\",sep=';',encoding='utf-8')\n",
    "    \n",
    "    if param==\"R5\":\n",
    "        with open(chemin1+\"MODELE.pickle\",'wb') as f:\n",
    "            cPickle.dump(m5,f)\n",
    "        export=proba_reco(proba5,cible)\n",
    "        print(export)\n",
    "        export.to_csv(chemin1+cible+\"_reco_proba.csv\",sep=';',encoding='utf-8')\n",
    "    \n",
    "    if param==\"R6\":\n",
    "        with open(chemin1+\"MODELE.pickle\",'wb') as f:\n",
    "            cPickle.dump(m6,f)\n",
    "        export=proba_reco(proba6,cible)\n",
    "        print(export)\n",
    "        export.to_csv(chemin1+cible+\"_reco_proba.csv\",sep=';',encoding='utf-8')\n",
    "    \n",
    "    if param==\"R7\":\n",
    "        with open(chemin1+\"M0.pickle\",'wb') as f:\n",
    "            cPickle.dump(m0,f)\n",
    "        with open(chemin1+\"M1.pickle\",'wb') as f:\n",
    "            cPickle.dump(m1,f)  \n",
    "        with open(chemin1+\"M2.pickle\",'wb') as f:\n",
    "            cPickle.dump(m2,f)\n",
    "        with open(chemin1+\"M3.pickle\",'wb') as f:\n",
    "            cPickle.dump(m3,f)\n",
    "        with open(chemin1+\"M4.pickle\",'wb') as f:\n",
    "            cPickle.dump(m4,f)\n",
    "        with open(chemin1+\"M5.pickle\",'wb') as f:\n",
    "            cPickle.dump(m5,f)\n",
    "        with open(chemin1+\"M6.pickle\",'wb') as f:\n",
    "            cPickle.dump(m6,f)\n",
    "        export=proba_reco(proba7,cible)\n",
    "        print(export)\n",
    "        export.to_csv(chemin1+cible+\"_reco_proba.csv\",sep=';',encoding='utf-8')\n",
    "        \n",
    "    \n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         R0\n",
      "9  1.375182\n",
      "0.083044982699\n",
      "OTOTOFOFOFO\n",
      "0.742468415938\n",
      "0.285714285714\n",
      "         R1\n",
      "9  1.511193\n",
      "         R2\n",
      "9  1.375182\n",
      "0.083044982699\n",
      "OTOTOFOFOFO\n",
      "0.742468415938\n",
      "0.285714285714\n",
      "        R3\n",
      "9  1.06192\n",
      "         R4\n",
      "9  1.092056\n",
      "         R5\n",
      "9  1.536968\n",
      "         R6\n",
      "9  1.456075\n",
      "         R7\n",
      "9  1.294289\n",
      "         R8\n",
      "9  1.577414\n",
      "9    0.344232\n",
      "Name: proba_min_decile, dtype: float64\n",
      "Index([u'R5'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "modelisation(X,y,'cible_un_mois','/home/foueslat/projet-encours/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contactid                 4114\n",
       "campagne                  4113\n",
       "campaign                  4113\n",
       "civilite                  4114\n",
       "code_postal               4113\n",
       "country_connexion_name    4113\n",
       "csp                       4114\n",
       "date_naissance            4113\n",
       "debit_cb                  3619\n",
       "flag_banque_principale    4114\n",
       "flag_epargne              4114\n",
       "mailing_accord            4113\n",
       "namesponsor               2288\n",
       "nature_cb                 3619\n",
       "patrimoine                4114\n",
       "pays                      4113\n",
       "regime_matrimonial        4114\n",
       "revenus_annuels           4114\n",
       "cible_un_mois             4114\n",
       "dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "        parameters = {'n_estimators':[150,300,350], 'n_jobs':[-1]}\n",
    "        rfc= RandomForestClassifier(n_estimators=200)\n",
    "        m0=grid_search.GridSearchCV(rfc,parameters,cv=6,n_jobs=-1)\n",
    "        #m0 = m0.fit(X0_train,y0_train)\n",
    "        rfc_model = rfc.fit(X0_train,y0_train)\n",
    "        proba0 = pd.DataFrame(y0_test)\n",
    "        P = m0.predict_proba(X0_test)[:, 1]\n",
    "        proba0[probabilite] = P\n",
    "        R0 = mesure(proba0,cible)\n",
    "        R0 = pd.DataFrame(R0)\n",
    "        R0.rename(columns={cible:\"R0\"},inplace=True)\n",
    "        print(R0)\n",
    "        \n",
    "    except ValueError:\n",
    "        R0=pd.DataFrame([0],index=[9],columns=[\"R0\"])\n",
    "        print(R0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model,decomposition\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFE,SelectFromModel\n",
    "from sklearn import metrics\n",
    "from sklearn import  grid_search\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier,BaggingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    \n",
    "X0_train,X0_test,y0_train,y0_test=train_test_split(X,y,random_state=42)\n",
    "rfc= RandomForestClassifier(n_estimators=200)\n",
    "rfc_model = rfc.fit(X0_train,y0_train)\n",
    "df_cible_test = pd.DataFrame(y0_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = rfc_model.predict_proba(X0_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1029, 18), (3085, 18), (1029,), (3085,))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0_test.shape, X0_train.shape, y0_test.shape, y0_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ar = numpy.array([[1.1, 2, 3.3, 4], [2.7, 10, 5.4, 7], [5.3, 9, 1.5, 15]])\n",
    "df = pd.DataFrame(ar, index = ['a1', 'a2', 'a3'], columns = ['A', 'B', 'C', 'D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a1</th>\n",
       "      <td>1.1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a2</th>\n",
       "      <td>2.7</td>\n",
       "      <td>10</td>\n",
       "      <td>5.4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a3</th>\n",
       "      <td>5.3</td>\n",
       "      <td>9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A   B    C   D\n",
       "a1  1.1   2  3.3   4\n",
       "a2  2.7  10  5.4   7\n",
       "a3  5.3   9  1.5  15"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.,  10.,   9.])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, jaccard_similarity_score, precision_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't handle mix of binary and continuous",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-214-2cf97da2ae24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjaccard_similarity_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my0_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mjaccard_similarity_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minvalid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         raise ValueError(\"Can't handle mix of {0} and {1}\"\n\u001b[1;32m---> 82\u001b[1;33m                          \"\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Can't handle mix of binary and continuous"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "jaccard_similarity_score(y0_test, r[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74829931972789121"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y0_test, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[754,  28],\n",
       "       [231,  16]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y0_test, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5144858506683786"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y0_test, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36363636363636365"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y0_test, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10996563573883161"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y0_test, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.064777327935222673"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y0_test, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19 ,  0.255,  0.105, ...,  0.07 ,  0.195,  0.185])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.81 ,  0.19 ],\n",
       "       [ 0.745,  0.255],\n",
       "       [ 0.895,  0.105],\n",
       "       ..., \n",
       "       [ 0.93 ,  0.07 ],\n",
       "       [ 0.805,  0.195],\n",
       "       [ 0.815,  0.185]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'n_estimators':[150,300,350], 'n_jobs':[-1]}\n",
    "rfc= RandomForestClassifier()\n",
    "m0=grid_search.GridSearchCV(rfc,parameters,cv=6,n_jobs=-1)\n",
    "m0 = m0.fit(X0_train,y0_train)\n",
    "proba0=pd.DataFrame(y0_test)\n",
    "P = m0.predict_proba(X0_test)[:, 1]\n",
    "y_pred = m0.predict(X0_test)\n",
    "\n",
    "proba0[probabilite]= P\n",
    "R0=mesure(proba0,cible)\n",
    "R0=pd.DataFrame(R0)\n",
    "R0.rename(columns={cible:\"R0\"},inplace=True)\n",
    "print(R0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('f1 score ' + f1_score(y0_test, r))\n",
    "print('accuracy ' + accuracy_score(y0_test, r))\n",
    "print('precision_score  ' + precision_score(y0_test, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "campagne                          4113\n",
       "campaign                          4113\n",
       "civilite                          4114\n",
       "code_postal                       4113\n",
       "country_connexion_name            4113\n",
       "csp                               4114\n",
       "date_naissance                    4113\n",
       "debit_cb                          3619\n",
       "flag_banque_principale            4114\n",
       "flag_epargne                      4114\n",
       "mailing_accord                    4113\n",
       "namesponsor                       2288\n",
       "nature_cb                         3619\n",
       "patrimoine                        4114\n",
       "pays                              4113\n",
       "regime_matrimonial                4114\n",
       "revenus_annuels                   4114\n",
       "cible_un_mois                     4114\n",
       "CONTACT_ID                        4114\n",
       "Pays De Naissance                 4114\n",
       "Code Postal Ville de Naissance    4114\n",
       "code_postal_naissance             4114\n",
       "pays_naissance_iso                4114\n",
       "dtype: int64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contactid                 4114\n",
       "campagne                  4113\n",
       "campaign                  4113\n",
       "civilite                  4114\n",
       "code_postal               4113\n",
       "country_connexion_name    4113\n",
       "csp                       4114\n",
       "date_naissance            4113\n",
       "debit_cb                  3619\n",
       "flag_banque_principale    4114\n",
       "flag_epargne              4114\n",
       "mailing_accord            4113\n",
       "namesponsor               2288\n",
       "nature_cb                 3619\n",
       "patrimoine                4114\n",
       "pays                      4113\n",
       "regime_matrimonial        4114\n",
       "revenus_annuels           4114\n",
       "cible_un_mois             4114\n",
       "dtype: int64"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3085, 18), (1029, 18), (3085,), (1029,))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0_train.shape, X0_test.shape, y0_train.shape, y0_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.759961127308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[782,   0],\n",
       "       [247,   0]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X0_train, y0_train)\n",
    "y_pred = lr.predict(X0_test)\n",
    "print(accuracy_score(y0_test, y_pred))\n",
    "confusion_matrix(y0_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1029,)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961 0.731778425656\n"
     ]
    }
   ],
   "source": [
    "best = 1\n",
    "best_i = 0\n",
    "for i in xrange(1000):  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state =i)\n",
    "    lr = LogisticRegression()\n",
    "    y_pred = lr.fit(X_train, y_train).predict(X_test)\n",
    "    _ = accuracy_score(y_test, y_pred)\n",
    "    if _ < best :\n",
    "        best = _\n",
    "        best_i = i\n",
    "print best_i, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}